{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "workflow.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyN2skC2LucjUI+2RsIVZPpf",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cbaldassari/gmm_init/blob/main/workflow.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Workflow"
      ],
      "metadata": {
        "id": "iS0wyzAFsgjP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This repository contains the code to reproduce all the results reported in the paper Unsupervised EM Initialization for Mixture Models: A Complex Network Driven Approach for Modeling Financial Time Series."
      ],
      "metadata": {
        "id": "ljqHU6kFsTSs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Data preparation"
      ],
      "metadata": {
        "id": "nwy_2jz3u2Nn"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jAJr9O5WgJMm",
        "outputId": "1c77b811-ca41-4f27-fde8-8201d623cd24"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!pip install community\n",
        "!pip install python-louvain\n",
        "!pip install tsia\n",
        "!pip install networkx\n",
        "!pip install easydev\n",
        "!pip install colormap\n",
        "!pip install missingpy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rtNO352A4KKn",
        "outputId": "3aefb0af-672c-4820-d5a1-f01c8b0e187e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: missingpy==0.2.0 in /usr/local/lib/python3.7/dist-packages (0.2.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GS96NKg75CP2"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn import mixture\n",
        "import matplotlib.pyplot as plt\n",
        "import missingpy\n",
        "from missingpy import MissForest\n",
        "import statsmodels.api as sm\n",
        "from matplotlib import gridspec\n",
        "from numba import njit, prange\n",
        "from pyts.image import MarkovTransitionField\n",
        "import tsia.plot\n",
        "import tsia.markov\n",
        "import tsia.network_graph\n",
        "import community\n",
        "from community import community_louvain\n",
        "import networkx as nx\n",
        "from matplotlib.colors import to_hex\n",
        "from sklearn.mixture import GaussianMixture\n",
        "from scipy.stats import kurtosis, skew\n",
        "import csv\n",
        "from colormap import rgb2hex\n",
        "from pickle import FALSE"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Gap Filling, Trend Removal and Log-return transformation"
      ],
      "metadata": {
        "id": "MmnxMx1gsmbn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "hubfilled=pd.DataFrame()\n",
        "hubnames=[\"gas\",\"oil\",\"spx\",\"pjm\"]\n",
        "stochastic2=pd.DataFrame()\n",
        "stochastic3=pd.DataFrame()\n",
        "stochastic4=pd.DataFrame()\n",
        "logprezzi=pd.DataFrame()\n",
        "trend=pd.DataFrame()\n",
        "prezzi=pd.DataFrame()\n",
        "flagin=True\n",
        "stocha=[]\n",
        "chart=pd.DataFrame()\n",
        "logprezzidepurati=pd.DataFrame()\n",
        "r=pd.DataFrame()\n",
        "x=pd.DataFrame()\n",
        "\n",
        "for hub in hubnames:\n",
        "  huball=pd.read_excel(\"market_data/\"+hub+\".xls\",sheet_name=\"Data 1\",parse_dates=[\"Trade date\"])\n",
        "  huball.rename(columns={'Wtd avg price $/MWh':'Price',  'Trade date':'Date'},inplace=True)\n",
        "  huballhub=huball.loc[huball['hub_transcode'] == hub]\n",
        "  a=len(huballhub)\n",
        "  huballhub.drop([\"hub_transcode\"],axis=1, inplace=True)\n",
        "  huballhub=huballhub.groupby('Date').first()\n",
        "  minrangedate=huballhub.index.min()\n",
        "  maxrangedate=huballhub.index.max()\n",
        "  check=pd.Series(pd.date_range(start = (minrangedate), end = maxrangedate, freq='D'))\n",
        "  check.sort_index(ascending=True)\n",
        "  #cnt=0\n",
        "  huballhub = huballhub.reindex(check, fill_value=np.NaN)\n",
        "  huballhub.index.name = \"Date\"\n",
        "  idxs=np.arange(huballhub.shape[0])##\n",
        "  toimpute=pd.DataFrame()\n",
        "  toimpute[\"Date\"]=idxs\n",
        "  toimpute[\"Price\"]=huballhub[\"Price\"].values\n",
        "  from missingpy import MissForest\n",
        "  imputer = MissForest(random_state=1234)\n",
        "  X_imputed = imputer.fit_transform(toimpute.to_numpy(),)\n",
        "  huballhub[\"Price\"]=X_imputed[:,1]\n",
        "  df=huballhub\n",
        "  start = '01-01-2015'\n",
        "  end = '31-12-2019'\n",
        "  mask = (huballhub.index >= start) & (huballhub.index <= end)\n",
        "  huballhub = huballhub.loc[mask]  \n",
        "  hubfilled[hub]=huballhub[\"Price\"]\n",
        "  chart[hub]=hubfilled[hub]\n",
        "  lowess = sm.nonparametric.lowess(np.log(hubfilled[hub]), hubfilled.index, frac=0.1)  \n",
        "  stocha=np.log(hubfilled[hub])-lowess[:, 1]\n",
        "  prezzi[hub]=hubfilled[hub]\n",
        "  logprezzidepurati[hub]=np.log(hubfilled[hub])-lowess[:, 1]\n",
        "  logprezzi[hub]=np.log(hubfilled[hub])\n",
        "  if (flagin):\n",
        "    trend=trend.reindex_like(prezzi)\n",
        "    flagin=False\n",
        "  trend[hub]=lowess[:, 1]\n",
        "  stochastic3[hub]=hubfilled[hub]\n",
        "  stochastic4[hub]=sm.nonparametric.lowess(hubfilled[hub], hubfilled.index, frac=0.1)[:, 1]\n",
        "  stochastic2[hub]=stocha-stocha.shift(1)\n",
        "  x[hub]=stocha\n",
        "  r[hub]=stochastic2[hub]\n",
        "stochastic2.dropna(inplace=True) "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d4OdAPbE1f5H",
        "outputId": "e4e7ba94-47a9-42d6-ad56-d09b72537d53"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration: 0\n",
            "Iteration: 1\n",
            "Iteration: 2\n",
            "Iteration: 0\n",
            "Iteration: 1\n",
            "Iteration: 2\n",
            "Iteration: 0\n",
            "Iteration: 1\n",
            "Iteration: 2\n",
            "Iteration: 0\n",
            "Iteration: 1\n",
            "Iteration: 2\n",
            "Iteration: 3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Some helper functions"
      ],
      "metadata": {
        "id": "FNYsh6z3vGxV"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vKbzzXUHFiPJ"
      },
      "source": [
        "# Useful constants definition\n",
        "COLORMAP = 'jet'\n",
        "\n",
        "def get_network_graph_f(mtf):\n",
        "    # Build the graph with networkx:\n",
        "    graph = nx.from_numpy_matrix(mtf)\n",
        "    \n",
        "    # Loops through the edges to get associate each of them with the\n",
        "    # corresponding Markov transition probability:\n",
        "    weights = [mtf[u,v] for u,v in graph.edges()]\n",
        "    for index, e in enumerate(graph.edges()):\n",
        "        graph[e[0]][e[1]]['weight'] = weights[index]\n",
        "        \n",
        "    return graph\n",
        "    \n",
        "def compute_network_graph_statistics_f(partitions, graph=None, mtf=None):    \n",
        "    if (graph is None) and (mtf is not None):\n",
        "        graph = get_network_graph(mtf)\n",
        "        \n",
        "    #partitions = community_louvain.best_partition(graph, random_state=1234)\n",
        "    nb_partitions = len(set(partitions.values()))\n",
        "    modularity = community_louvain.modularity(partitions, graph)\n",
        "\n",
        "    \n",
        "    diameter = nx.diameter(graph)\n",
        "    node_size = list(nx.clustering(graph, weight='weight').values())\n",
        "    avg_clustering_coeff = np.array(node_size).mean()\n",
        "    density = nx.density(graph)\n",
        "    avg_path_length = nx.average_shortest_path_length(graph, weight='weight', method='dijkstra')\n",
        "    \n",
        "    average_degree = nx.average_degree_connectivity(graph)\n",
        "    average_degree = np.mean(list(average_degree.values()))\n",
        "    avg_weighted_degree = nx.average_degree_connectivity(graph, weight='weight')\n",
        "    avg_weighted_degree = np.mean(list(avg_weighted_degree.values()))\n",
        "    \n",
        "    statistics = {\n",
        "        'Diameter': diameter,\n",
        "        'Average degree': average_degree,\n",
        "        'Average weighted degree': avg_weighted_degree,\n",
        "        'Density': density,\n",
        "        'Average path length': avg_path_length,\n",
        "        'Average clustering coefficient': avg_clustering_coeff,\n",
        "        'Modularity': modularity,\n",
        "        'Partitions': nb_partitions\n",
        "    }\n",
        "    \n",
        "    return statistics\n",
        "    \n",
        "def get_modularity_encoding_f(graph, colormap=COLORMAP, reversed_cmap=False):\n",
        "  \n",
        "    if reversed_cmap == True:\n",
        "        colormap = plt.cm.get_cmap(colormap).reversed()\n",
        "    else:\n",
        "        colormap = plt.cm.get_cmap(colormap)\n",
        "    \n",
        "    # Get the node partitions and number of partitions found with the Louvain\n",
        "    # algorithm, as implemented in the `community` package:\n",
        "\n",
        "    partitions = community_louvain.best_partition(graph, random_state=1234)\n",
        "    #####################################\n",
        "    \n",
        "    nb_partitions = len(set(partitions.values()))\n",
        "    #print(\"nb_partitions: \",nb_partitions)\n",
        "\n",
        "    # Compute node colors and edges colors for the modularity encoding:\n",
        "    edge_colors = [to_hex(colormap(partitions.get(v)/(nb_partitions - 1))) for u,v in graph.edges()]\n",
        "    node_colors = [partitions.get(node) for node in graph.nodes()]\n",
        "    node_size = list(nx.clustering(graph, weight='weight').values())\n",
        "    node_size = list((node_size - np.min(node_size)) * 2000 + 10)\n",
        "    \n",
        "    # Store the encoding to return in a dictionnary:\n",
        "    #print(\"node_colors: \",len(set(node_colors)))\n",
        "\n",
        "    encoding = {\n",
        "        'node_size': node_size,\n",
        "        'edge_color': edge_colors,\n",
        "        'node_color': node_colors\n",
        "    }\n",
        "    return encoding, partitions\n",
        "\n",
        "def stat_f(w,m,v):\n",
        "  x2=[]\n",
        "  x3=[]\n",
        "  x4=[]\n",
        "  n=len(w)\n",
        "  for j in range(n):\n",
        "    x2.append(v[j]+m[j]**2)\n",
        "    x3.append(pow(m[j],3)+3*m[j]*v[j])\n",
        "    x4.append(pow(m[j],4)+6*m[j]**2*v[j]+3*v[j]**2)\n",
        "  X1=np.dot(w,m)\n",
        "  X2=np.dot(w,x2)\n",
        "  X3=np.dot(w,x3)\n",
        "  X4=np.dot(w,x4)\n",
        "  mu=X1\n",
        "  sig=np.sqrt(np.subtract(X2, mu**2))\n",
        "  sk=(X3-3*X2*X1+2*pow(X1,3))/pow(sig,3)\n",
        "  kur=(X4-4*X3*X1+6*X2*X1**2-3*pow(X1,4))/pow(sig,4)\n",
        "  return [mu, sig, sk, kur]\n",
        "\n",
        "def get_network_graph_map_f(timeseries, encoding, colormap=COLORMAP, reversed_cmap=False):\n",
        "   \n",
        "    # Get encoding definitions:\n",
        "    node_colors = encoding['node_color']\n",
        "\n",
        "    #print(node_colors)\n",
        "\n",
        "    image_size = len(node_colors)\n",
        "    #print(\"node_colors\",node_colors)\n",
        "    #print(\"np.max(node_colors)\",np.max(node_colors))\n",
        "    partition_color = node_colors / np.max(node_colors)\n",
        "\n",
        "    # Define the color map:\n",
        "    if reversed_cmap == True:\n",
        "        colormap = plt.cm.get_cmap(colormap).reversed()\n",
        "    else:\n",
        "        colormap = plt.cm.get_cmap(colormap)\n",
        "\n",
        "    # Plot each subset of the signal with the color associated to the network\n",
        "    # graph partition it belongs to:\n",
        "    network_graph_map = []\n",
        "    sequences_width = timeseries.shape[0] / image_size\n",
        "\n",
        "    #df=pd.DataFrame([{\"color\": p ,\"value\": k}])\n",
        "\n",
        "    for i in range(image_size):\n",
        "        c = colormap(partition_color[i])\n",
        "\n",
        "        start = int(i * sequences_width)\n",
        "        end = int((i+1) * sequences_width)#-1\n",
        "        data = timeseries.iloc[start:end, :]\n",
        "\n",
        "        current_map = dict()\n",
        "\n",
        "        current_map.update({\n",
        "            'color': c,\n",
        "            'slice': data\n",
        "        })\n",
        "\n",
        "        #print(len(current_map[\"slice\"]))\n",
        "\n",
        "        network_graph_map.append(current_map)\n",
        "        \n",
        "    return network_graph_map, node_colors\n",
        "\n",
        "\n",
        "def inversemapAna(ng_map2,colors2):\n",
        "  df=pd.DataFrame(columns=[\"color\",\"value\"])\n",
        "  dout=pd.DataFrame(columns=[\"color\",\"value\"])\n",
        "  for i in range(len(ng_map2)):\n",
        "      d=ng_map2[i]\n",
        "      p=colors2[i]\n",
        "      slic=d[\"slice\"].values.reshape(-1)\n",
        "\n",
        "      for k in slic:\n",
        "        df=df.append([{\"color\": p ,\"value\": k}], ignore_index=True)\n",
        "  \n",
        "  df[\"diff\"]=df[\"value\"]-df[\"value\"].shift(1)\n",
        "  df.drop(df.index[[0]], inplace=True)\n",
        "  df.drop(['value'], axis = 1, inplace=True)\n",
        "  df.rename(columns = {'diff':'value'}, inplace = True)\n",
        "  return df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Distribution estimation"
      ],
      "metadata": {
        "id": "tvaNQT9LvL2j"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Markov Transition Fields, Network association, Network stats, Gaussian mixture model and Optimization Grid "
      ],
      "metadata": {
        "id": "9mn0AKcmvVsX"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a1QNfDwhRfkT"
      },
      "source": [
        "hubnames=[\"pjm\",\"gas\",\"oil\",\"spx\"]\n",
        "for hub in hubnames:\n",
        "    r = pd.read_pickle(\"returns.pk\")\n",
        "    rr=r[hubnames].dropna()\n",
        "    flagFirst=True\n",
        "    strategy = 'quantile'\n",
        "    note=pd.DataFrame(columns=[\"hub\",\"bins\",\"imsize\",\"bic\",\"aic\"])\n",
        "    flag=True\n",
        "    gridlist = []\n",
        "    for b in range(2,55,2):\n",
        "      for ts in range(5,405,5):\n",
        "        gridlist.append((b,ts))\n",
        "    gridlist = tuple(gridlist)\n",
        "\n",
        "    ccc=0\n",
        "    tag_data = tag_data = pd.read_pickle(\"logdetrend.pk\")\n",
        "    tag_df=tag_data[hub]\n",
        "    for g in gridlist:\n",
        "      bins=g[0]\n",
        "      imsize=g[1]\n",
        "\n",
        "    X = tag_df.values.reshape(1, -1)\n",
        "\n",
        "    mtf = MarkovTransitionField(image_size=imsize, n_bins=bins, strategy=strategy,overlapping=False)\n",
        "    tag_mtf = mtf.fit_transform(X)\n",
        "    \n",
        "    graph= get_network_graph_f(tag_mtf[0])\n",
        "    encoding2, partitions = get_modularity_encoding_f(graph)\n",
        "    \n",
        "    ccc+=1\n",
        "    if (len(set(partitions.values()))<=10):\n",
        "      tag_df=pd.DataFrame(tag_df)\n",
        "    \n",
        "      ng_map2, colors2 = get_network_graph_map_f(tag_df, encoding2)\n",
        "      statistics=compute_network_graph_statistics_f(partitions,graph)\n",
        "\n",
        "      nb_partitions=statistics[\"Partitions\"]\n",
        "      modularity=statistics[\"Modularity\"]\n",
        "\n",
        "      lin_map=inversemapAna(ng_map2,colors2)      \n",
        "      tag_df=rr\n",
        "\n",
        "      colors=lin_map.groupby(['color']).size()\n",
        "      colors=colors.index\n",
        "\n",
        "      means=[]\n",
        "      precisions=[]\n",
        "      nk=[]\n",
        "        \n",
        "      for c in colors:\n",
        "        a=lin_map.value[lin_map[\"color\"]==c].values\n",
        "        means.append(np.mean(a))\n",
        "        precisions.append(1/pow(np.std(a),2))\n",
        "        nk.append(len(a)/len(lin_map))\n",
        "\n",
        "      precisions=np.array(precisions).reshape(-1,1,1)\n",
        "      means=np.array(means).reshape(-1,1)\n",
        "\n",
        "      grid=pd.DataFrame(columns=[\"idxs\",\"hub\",\"aic\",\"bic\",\"comp\",\"weights\",\"means\",\"covariances\"])\n",
        "      grid.set_index(\"idxs\")     \n",
        "      \n",
        "      itemorig={\"1\":tag_df[hub].mean(),\n",
        "            \"2\":tag_df[hub].std(),\n",
        "            \"3\":skew(tag_df[hub]),\n",
        "            \"4\":kurtosis(tag_df[hub])+3\n",
        "      }\n",
        "\n",
        "      XY = tag_df[hub].values.reshape(-1, 1)\n",
        "\n",
        "      try:\n",
        "          gmm = GaussianMixture(n_components=len(nk), weights_init=nk, means_init=means, precisions_init=precisions, covariance_type='full').fit(XY)\n",
        "      except ValueError:      \n",
        "        continue\n",
        "        \n",
        "      nosim=foo(gmm.weights_.reshape(-1),gmm.means_.reshape(-1),gmm.covariances_.reshape(-1))\n",
        "\n",
        "      grid.at[0,'comp']=nb_partitions\n",
        "      grid.at[0,'hub']=hub\n",
        "      grid.at[0,'comp']=gmm.n_components\n",
        "\n",
        "      grid.at[0,'bins']=bins\n",
        "      grid.at[0,'imsize']=imsize\n",
        "      grid.at[0,'netstat']=str(statistics)\n",
        "\n",
        "      grid.at[0,'bic']=gmm.bic(XY) \n",
        "      grid.at[0,'aic']=gmm.aic(XY)\n",
        "      grid.at[0,'weights']=gmm.weights_.reshape(-1)\n",
        "      grid.at[0,'means']=gmm.means_.reshape(-1)\n",
        "      grid.at[0,'covariances']=gmm.covariances_.reshape(-1)\n",
        "\n",
        "      grid.at[0,'orig_M2']=itemorig[\"2\"]\n",
        "      grid.at[0,'orig_M3']=itemorig[\"3\"]\n",
        "      grid.at[0,'orig_M4']=itemorig[\"4\"]\n",
        "\n",
        "      grid.at[0,'GMM_M2']=nosim[1]\n",
        "      grid.at[0,'GMM_M3']=nosim[2]\n",
        "      grid.at[0,'GMM_M4']=nosim[3]\n",
        "\n",
        "      grid.at[0,'absdiff_M2']=abs(itemorig[\"2\"]-nosim[1])\n",
        "      grid.at[0,'absdiff_M3']=abs(itemorig[\"3\"]-nosim[2])\n",
        "      grid.at[0,'absdiff_M4']=abs(itemorig[\"4\"]-nosim[3])\n",
        "\n",
        "      grid.at[0,'rel%diff_M2']=100*abs((itemorig[\"2\"]-nosim[1])/(itemorig[\"2\"]))\n",
        "      grid.at[0,'rel%diff_M3']=100*abs((itemorig[\"3\"]-nosim[2])/(itemorig[\"3\"]))\n",
        "      grid.at[0,'rel%diff_M4']=100*abs((itemorig[\"4\"]-nosim[3])/(itemorig[\"4\"]))\n",
        "      \n",
        "      if (flag):\n",
        "        grid.to_csv(\"/content/drive/MyDrive/Mari/plotpaper2/x_r_\"+strategy+\"_MTFgrid_\"+hub.upper()+\".csv\", header=True, mode='a')\n",
        "        flag=False\n",
        "      else:\n",
        "        grid.to_csv(\"/content/drive/MyDrive/Mari/plotpaper2/x_r_\"+strategy+\"_MTFgrid_\"+hub.upper()+\".csv\", header=False, mode='a')\n",
        "\n",
        "      print(bins,\"#\",imsize)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}